{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MPT model using Mosaic composer and Amazon SageMaker.\n",
    "\n",
    "\n",
    "We will start with upgrading SageMaker Python SDK and boto3. Followed by sagemaker imports and session creation required to launch the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and upload dataset to S3\n",
    "\n",
    "We will download the c4 small dataset , convert it to streaming format and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python scripts/convert_dataset_hf.py \\\n",
    "    --dataset c4 \\\n",
    "    --data_subset en \\\n",
    "    --out_root data/my-copy-c4 \\\n",
    "    --splits train_small val_small \\\n",
    "    --concat_tokens 2048 \\\n",
    "    --tokenizer EleutherAI/gpt-neox-20b \\\n",
    "    --eos_text '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = sess.upload_data(\n",
    "    path=\"data\",\n",
    "    key_prefix=\"dataset/c4small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data uploaded here - {train_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Yaml file remote path with the above S3 URL. For this job we will use the mpt-7b.yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training job using the Custom Docker Image\n",
    "\n",
    "Update he image_uri in the estimator below to use the custom image that we built.As mentioned in the beginning, we will use Amazon SageMaker and Mosaic Composer to train our model. Amazon SageMaker makes it easy to create a multi-node cluster to train our model in a distributed manner. The sagemaker python SDK supports to run training jobs using torchrun, to distribute the script across multiple nodes and GPUs.\n",
    "\n",
    "To use torchrun to execute our scripts, we only have to define the distribution parameter in our Estimator and set it to \"torch_distributed\": {\"enabled\": True}. This tells sagemaker to launch our training job with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# define Training Job Name \n",
    "job_name = f'mosaic-llmfoundry-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# This environment variables are useful when training with P4d inorder to enable EFA based training.\n",
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "\n",
    "hyperparameters = {}\n",
    "hyperparameters[\"config_path\"] = \"yamls\"\n",
    "hyperparameters[\"config_name\"] = \"mpt-7b.yaml\"\n",
    "hyperparameters[\"backend\"] = \"nccl\" # Use smddp when you scale cluster size for better performance.\n",
    "# estimator \n",
    "pt_estimator = PyTorch(\n",
    "    entry_point='run.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    image_uri=\"xxxx.dkr.ecr.us-west-2.amazonaws.com/mosaic-llm-foundry-dlc:latest\",\n",
    "    instance_count=2,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    environment=env,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate warmpools when not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(pt_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
